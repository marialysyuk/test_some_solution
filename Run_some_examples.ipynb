{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2847fb-2c8f-4b39-b6a5-082bdd4226f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \\\n",
    "#   --local-dir models/qwen2.5-7b-instruct-awq \\\n",
    "#   --local-dir-use-symlinks False\n",
    "\n",
    "# ! huggingface-cli download FractalGPT/RuQwen2.5-3B-Instruct-AWQ \\\n",
    "#   --local-dir models/RuQwen2.5-3B-Instruct-AWQ \\\n",
    "#   --local-dir-use-symlinks False\n",
    "\n",
    "# ! huggingface-cli download hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 \\\n",
    "#   --local-dir ./llama3.1-8b-instruct-awq \\\n",
    "#   --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc802833-73f1-483f-b179-80d1591ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install autoawq transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d337edf-5260-4207-acdb-02fef8ad73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"s-nlp/EverGreen-Multilingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "298896ce-379d-48d8-9d1c-06c97d7d916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "arr = pd.DataFrame(df['train'])[\"Russian\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ce3f956-6a45-4595-a319-95f89ebf937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [str(x) for x in arr.tolist()]\n",
    "import json\n",
    "with open(\"input.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66341d37-6b51-4952-a6bd-055bd8814617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452d27f0-3ba9-491e-afe3-654173603962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.8.*, but PyTorch 2.5.1+cu124 is found. Please switch to the matching version and run again.\n",
      "ERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.8.*, but PyTorch 2.5.1+cu124 is found. Please switch to the matching version and run again.\n",
      "Replacing layers...: 100%|██████████████████████| 32/32 [00:02<00:00, 12.76it/s]\n",
      "/home/jovyan/.mlspace/envs/eedi/lib/python3.9/site-packages/accelerate/utils/imports.py:360: UserWarning: Intel Extension for PyTorch 2.8 needs to work with PyTorch 2.8.*, but PyTorch 2.5.1 is found. Please switch to the matching version and run again.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/eedi/lib/python3.9/site-packages/awq/models/base.py:520: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
      "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n",
      "  0%|                                                   | 0/251 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jovyan/.mlspace/envs/eedi/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|█████████████████████████████████████████| 251/251 [11:15<00:00,  2.69s/it]\n",
      "Wrote 2008 answers to output.json\n"
     ]
    }
   ],
   "source": [
    "! python solution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47b5efb3-2603-47d4-a1b1-bbd0b4f9ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('input.json', 'r') as file:\n",
    "    input_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "004173c6-5105-4514-b619-bdbd704cea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.json', 'r') as file:\n",
    "    output_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b3a5665-e226-4efb-ae0a-7358326f54bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вопрос некорректен\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from awq import AutoAWQForCausalLM\n",
    "# import torch\n",
    "\n",
    "# MODEL_DIR = \"./llama3.1-8b-instruct-awq\"\n",
    "\n",
    "# model = AutoAWQForCausalLM.from_quantized(\n",
    "#     MODEL_DIR, device_map=\"auto\", fuse_layers=True,\n",
    "#     torch_dtype=torch.float16, trust_remote_code=True\n",
    "# )\n",
    "# tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Отвечай ТОЛЬКО по-русски и ТОЛЬКО коротким фактом.\\\n",
    "#     Не перефразируй вопрос, в ответе должен быть ТОЛЬКО короткий ответ.\\\n",
    "#     Если тебе задали некорректный вопрос отвечай 'вопрос некорректен'.\\\n",
    "#     Если ты очень не уверена в ответе, отвечай 'не знаю'.\"},\n",
    "#     {\"role\": \"user\", \"content\": ' Когда Ангела Меркель была прездидентом США?'},\n",
    "# ]\n",
    "\n",
    "# # apply_chat_template returns a Tensor for a single example\n",
    "# input_ids = tok.apply_chat_template(\n",
    "#     messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "# device = next(model.parameters()).device\n",
    "# input_ids = input_ids.to(device)\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     out = model.generate(\n",
    "#         input_ids=input_ids,            # <-- pass as a kwarg\n",
    "#         do_sample=False, temperature=0.0, top_p=1.0,\n",
    "#         max_new_tokens=16,\n",
    "#         pad_token_id=tok.eos_token_id, eos_token_id=tok.eos_token_id,\n",
    "#     )\n",
    "\n",
    "# gen_ids = out[0, input_ids.size(1):]\n",
    "# print(tok.decode(gen_ids, skip_special_tokens=True).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b10c2-8bb9-4f8c-a8ac-842a72b03ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-eedi]",
   "language": "python",
   "name": "conda-env-.mlspace-eedi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
